# Issue v0.2.1: Webページ取得ツールの実装

## 前提知識
- `docs/Architecture/Architecture.md`

## 現状の問題点
- `tools/web_fetch.py`に不備が存在する。
- **出力の不備**: 現在stdoutに変換結果のHTMLコードをすべて出力してしまっているが、コンテクストが大きすぎてエージェントの負担が過大である。

## 解決策の概要
- `web_fetch.py` を修正する。
- 一時ファイルの出力先であるディレクトリを引数で指定できるようにする　
- 標準出力には結果の成否と変換した文書の数、かかった時間などをレポートとして出力する

## 解決策の詳細
`web_fetch.py` は、以下の仕様を満たすツールとして修正される。

1.  **入力**:
    - `--url`: 取得対象のWebページのURL。
    - `--base-url`: ドキュメントの範囲を定義するためのベースURL。このURLのパス以下のみをドキュメントとして扱う。LLMが推測して渡すことを想定。
    - **`--temp_dir`:(追加)** 変換結果のHTML文書を一時的に出力するディレクトリ
    - `--recursive` (オプション): ページ内のリンクを再帰的に探索し、コンテンツを収集するかどうか（デフォルト: True）。
    - `--depth` (オプション): 再帰探索の最大深度（デフォルト: 5）。
2.  **コンテンツ取得**:
    - `requests` ライブラリを使用して、指定されたURLのHTMLコンテンツを取得する。
    - **主要コンテンツの抽出とノイズ除去**: 取得したHTMLから、広告、ナビゲーション、フッターなどの不要な要素を除去し、記事本文などの主要コンテンツのみを抽出する。`trafilatura` ライブラリを主要なツールとして活用する。**Node.js環境の問題により、`Readability.js` のようなNode.jsベースのツールは使用しないこと。**
    - **リンク解析とドキュメント範囲判定**: ページ内のリンク解析には `BeautifulSoup` を使用する。リンクの探索は、`--base-url` で指定されたパス以下に限定されるべきである。
    - 再帰探索が有効な場合、ページ内のリンクを解析し、指定された深度までコンテンツを収集する。
    - 収集したHTMLコンテンツは、後続の `html_to_markdown.py` で処理されることを想定する。
3.  **結果の出力**: 収集したWebページコンテンツは一時ディレクトリに保存される。一時ディレクトリ内には、各WebページのHTMLコンテンツファイルと、それらのURLとファイルパスをマッピングする`discovery.json`ファイルが含まれる。
    ```json
    {
      "status": "success",
      "output_dir": "/path/to/temp_output_directory",
      "converted_count": 15,
      "depth_level": 3
    }
    ```
    `output_directory`内の`discovery.json`の例:
    ```json
    [
      {
        "url": "https://example.com/page1",
        "file_path": "page1.html"
      },
      {
        "url": "https://example.com/page2",
        "file_path": "page2.html"
      }
    ]
    ```
4.  **作法**:
  - エラーハンドリング（HTTPエラー、タイムアウト、接続エラーなど）や引数解析は、既存のツール（例: 旧 `make_vector_db.py`）の堅牢な作法を踏襲する。
  - 文書レベル、クラスレベル、メソッドレベルそれぞれにエージェントがコードを読まずとも理解するに足りるだけのDocstringsを用意する。
  - `samples/tool_sample.py`: エラーハンドリング、引数解析、JSON出力形式などの作法を参考に。

## テスト方法
`web_fetch.py` の機能が正しく動作することを確認するため、以下のテストを実施する。

1.  **テストURLの準備**: 取得可能なWebページのURLを用意する。
2.  **ツール実行**: `web_fetch.py` を、テストURLを引数として実行する。
    - 例: `python web_fetch.py --url "https://example.com" --base-url "https://example.com/docs/"`
    - 例: `python web_fetch.py --url "https://example.com" --base-url "https://example.com/docs/" --recursive --depth 2`
3.  **出力確認**: 
    - 標準出力に、`status: success` を含むJSONが返されることを確認する。
    - `fetched_pages` 配列に、期待されるURLとHTMLコンテンツが含まれていることを確認する。
4.  **ドキュメント範囲判定の検証**:
    -   **テストケースの準備**:
        -   ベースURLのパス以下にのみリンクが存在するページ。
        -   ベースURLのパス以外にもリンクが存在するが、それらは収集されないことを確認するページ。
    -   **ツール実行と結果の確認**:
        -   `--base-url` を指定して `web_fetch.py` を実行し、`fetched_pages` に含まれるURLが `--base-url` のパス以下に限定されていることを確認する。
5.  **コンテンツ抽出とノイズ除去の検証**:
        -   **テストケースの準備**:
        -   純粋なドキュメントページ: url = `https://jules.google/docs`, base-url = `https://jules.google/docs`
        -   技術ブログ記事など、広告、サイドバー、関連リンクなどのノイズが多いページ（例: [具体的なURLをここに指定]）
    -   **ツール実行と結果の確認**:
        -   上記テストケースのURLに対して `web_fetch.py` を実行し、`fetched_pages` 内の `html_content` が、主要な記事本文のみを含み、不要なノイズが適切に除去されていることを目視またはプログラム的に確認する。
        -   `trafilatura` ライブラリの有効性を検証し、必要に応じて `BeautifulSoup` を用いたHTML構造解析も組み合わせることで、主要コンテンツが適切に抽出されることを確認する。
        -   特に、`html_to_markdown.py` で処理された際に、RAGのソースとして最適なマークダウンが生成されるかを確認する。

## jules向けの注意点
ユーザーとの対話には日本語を使用すること！！