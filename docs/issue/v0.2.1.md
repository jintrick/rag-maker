# Issue v0.2.1: Webページ取得ツールの実装

## 前提知識
- `docs/Architecture/Architecture.md`

## 現状の問題点
- `/rag` コマンドがWebページから情報を収集する際、指定されたURLのコンテンツを効率的かつ堅牢に取得するツールが存在しない。
- リンクの再帰的な探索や、エラーハンドリング（タイムアウト、接続エラーなど）を考慮する必要がある。

## 解決策の概要
- 指定されたURLのWebページコンテンツを収集するPythonスクリプト `web_fetch.py` を作成する。
- このツールは、`/rag` コマンドのWebページからのデータ収集フローの中核を担う。

## 解決策の詳細
`web_fetch.py` は、以下の仕様を満たすツールとして実装される。

1.  **入力**:
    - `--url`: 取得対象のWebページのURL。
    - `--recursive` (オプション): ページ内のリンクを再帰的に探索し、コンテンツを収集するかどうか（デフォルト: True）。
    - `--depth` (オプション): 再帰探索の最大深度（デフォルト: 5）。
2.  **コンテンツ取得**:
    - `requests` および `BeautifulSoup` ライブラリを使用して、指定されたURLのHTMLコンテンツを取得する。
    - **主要コンテンツの抽出とノイズ除去**: 取得したHTMLから、広告、ナビゲーション、フッターなどの不要な要素を除去し、記事本文などの主要コンテンツのみを抽出する。`BeautifulSoup` を用いたHTML構造解析、または `ReadabiliPy` のような「可読性」ライブラリの活用を検討する。
    - 再帰探索が有効な場合、ページ内のリンクを解析し、指定された深度までコンテンツを収集する。
    - 収集したHTMLコンテンツは、後続の `html_to_markdown.py` で処理されることを想定する。
3.  **結果の出力**: 収集したWebページコンテンツのリストを、JSON形式で標準出力に返す。各エントリには、URLとHTMLコンテンツを含める。
    ```json
    {
      "status": "success",
      "fetched_pages": [
        {
          "url": "...",
          "html_content": "..."
        },
        ...
      ]
    }
    ```
4.  **作法**: エラーハンドリング（HTTPエラー、タイムアウト、接続エラーなど）や引数解析は、既存のツール（例: 旧 `make_vector_db.py`）の堅牢な作法を踏襲する。

## 参考とすべきスクリプト等
- (旧) `make_vector_db.py`: エラーハンドリング、引数解析、JSON出力形式などの作法を参考に。

## テスト方法
`web_fetch.py` の機能が正しく動作することを確認するため、以下のテストを実施する。

1.  **テストURLの準備**: 取得可能なWebページのURLを用意する。
2.  **ツール実行**: `web_fetch.py` を、テストURLを引数として実行する。
    - 例: `python web_fetch.py --url "https://example.com"`
    - 例: `python web_fetch.py --url "https://example.com" --recursive --depth 2`
3.  **出力確認**:
    - 標準出力に、`status: success` を含むJSONが返されることを確認する。
    - `fetched_pages` 配列に、期待されるURLとHTMLコンテンツが含まれていることを確認する。
4.  **コンテンツ抽出とノイズ除去の検証**:
        -   **テストケースの準備**:
        -   純粋なドキュメントページ: `https://jules.google/docs`
        -   純粋なドキュメントページ: `https://www.python.org/doc/`
        -   技術ブログ記事など、広告、サイドバー、関連リンクなどのノイズが多いページ（例: [具体的なURLをここに指定]）
    -   **ツール実行と結果の確認**:
        -   上記テストケースのURLに対して `web_fetch.py` を実行し、`fetched_pages` 内の `html_content` が、主要な記事本文のみを含み、不要なノイズが適切に除去されていることを目視またはプログラム的に確認する。
        -   `BeautifulSoup` を用いたHTML構造解析、`ReadabiliPy`、またはその他の「可読性」ライブラリなど、複数のアプローチを試行し、それぞれの効果と課題を報告すること。
        -   特に、`html_to_markdown.py` で処理された際に、RAGのソースとして最適なマークダウンが生成されるかを確認する。
